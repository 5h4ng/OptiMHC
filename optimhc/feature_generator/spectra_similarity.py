# feature_generator/spectra_similarity.py

import logging
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple, Union, Optional
import os
from scipy.stats import pearsonr, spearmanr
from scipy.spatial.distance import cosine
from optimhc.feature_generator.base_feature_generator import BaseFeatureGenerator
from optimhc import utils
from optimhc.parser import extract_mzml_data
from koinapy import Koina

logger = logging.getLogger(__name__)


class SpectraSimilarityFeatureGenerator(BaseFeatureGenerator):
    """
    Feature generator for calculating similarity between experimental and predicted spectra.

    This class works through the following steps:
    1. Extract experimental spectral data from mzML files
    2. Use Koina for theoretical spectra prediction
    3. Align experimental and predicted spectra
    4. Calculate similarity metrics as features

    Parameters:
        peptides (List[str]): List of peptide sequences
        charges (List[int]): List of charge states
        scan_ids (List[int]): List of scan IDs
        mz_file_paths (List[str]): List of mzML file paths
        model_type (str): Prediction model type, either "HCD" or "CID"
        collision_energies (List[float]): List of collision energies, required when model_type is "HCD"
        remove_pre_nxt_aa (bool): Whether to remove preceding and next amino acids, default is True
        remove_modification (bool): Whether to remove modifications, default is True
        url (str): Koina server URL, default is "koina.wilhelmlab.org:443"
        top_n (int): Number of top peaks to use for alignment, default is 12
        tolerance_ppm (float): Mass tolerance for alignment in ppm, default is 20
    """

    def __init__(
        self,
        spectrum_ids: List[str],
        peptides: List[str],
        charges: List[int],
        scan_ids: List[int],
        mz_file_paths: List[str],
        model_type: str,
        collision_energies: List[float] = None,
        instruments: List[str] = None,
        fragmentation_types: List[str] = None,
        remove_pre_nxt_aa: bool = False,
        mod_dict: Optional[Dict[str, str]] = None,
        url: str = "koina.wilhelmlab.org:443",
        top_n: int = 36,
        tolerance_ppm: float = 20,
    ):
        self.spectrum_ids = spectrum_ids
        self.peptides = peptides
        self.charges = charges
        self.scan_ids = scan_ids
        self.mz_file_paths = mz_file_paths
        self.model_type = model_type
        self.collision_energies = collision_energies
        self.instruments = instruments
        self.fragmentation_types = fragmentation_types
        self.remove_pre_nxt_aa = remove_pre_nxt_aa
        self.mod_dict = mod_dict
        self.url = url
        self.top_n = top_n
        self.tolerance_ppm = tolerance_ppm
        self.results = None
        self._raw_predictions = None

        logger.info(
            f"Initializing SpectraSimilarityFeatureGenerator with {len(peptides)} PSMs"
        )
        logger.info(f"Using model: {self.model_type}")

        self.df = pd.DataFrame(
            {
                "spectrum_id": self.spectrum_ids,
                "scan": self.scan_ids,
                "peptide": self.peptides,
                "charge": self.charges,
                "mz_file_path": self.mz_file_paths,
            }
        )

        self.df["processed_peptide"] = self.df["peptide"].apply(
            self._preprocess_peptide
        )
        logger.info(
            f"Recevied {len(self.df)} PSMs for spectral similarity feature generation"
        )

    @property
    def id_column(self) -> List[str]:
        """
        Returns a list of input columns required for the feature generator.
        """
        return ["spectrum_id", "peptide", "charge"]

    @property
    def feature_columns(self) -> List[str]:
        """
        Returns a list of feature columns generated by the feature generator.
        """
        return [
            "spectral_angle_similarity",
            "cosine_similarity",
            "pearson_correlation",
            "spearman_correlation",
            "mean_squared_error",
            "unweighted_entropy_similarity",
            "predicted_seen_nonzero",
            "predicted_not_seen",
        ]

    def input_df(self) -> pd.DataFrame:
        """
        Return the generated features as a DataFrame.

        Returns:
            pd.DataFrame: DataFrame containing the generated features
        """
        return self.df

    def _preprocess_peptide(self, peptide: str) -> str:
        """
        Preprocess peptide sequence.

        As Prosit does not support non-standard amino acid 'U', we replace it with 'C'.

        Parameters
        ----------
        peptide : str
            Original peptide sequence.

        Returns
        -------
        str
            Processed peptide sequence.

        Notes
        -----
        This is nonsense when it comes to spectral prediction, but we need to keep it
        for compatibility with Koina. In the future, this should be prohibited at the input level.
        """
        processed_peptide = peptide

        if self.remove_pre_nxt_aa:
            processed_peptide = utils.remove_pre_and_nxt_aa(processed_peptide)

        if self.mod_dict is None:
            processed_peptide = utils.remove_modifications(processed_peptide)
        else:
            for mod, replacement in self.mod_dict.items():
                processed_peptide = processed_peptide.replace(mod, replacement)

        # Replace non-standard amino acid 'U' with 'C'.
        # This is nosense when it comes to spectral prediction.
        # But we need to keep it for compatibility with Koina.
        # In the future, this should be prohibited at the input level.
        if "U" in utils.remove_modifications(processed_peptide):
            logger.warning(
                f"Peptide sequence contains non-standard amino acid 'U': {processed_peptide}. Replacing with 'C'."
            )
            # Replace 'U' with 'C'
            processed_peptide = processed_peptide.replace("U", "C")

        return processed_peptide

    def _extract_experimental_spectra(self) -> pd.DataFrame:
        """
        Extract experimental spectral data from mzML files.

        Returns
        -------
        pd.DataFrame
            DataFrame containing experimental spectral data.

        Notes
        -----
        The method groups scan IDs by file path for efficiency and extracts
        spectral data from each mzML file. The resulting DataFrame contains
        m/z values, intensities, and associated metadata for each spectrum.
        """
        logger.info("Extracting experimental spectral data...")

        # Group scan IDs by file path for efficiency
        file_to_scans = {}
        for i, row in self.df.iterrows():
            scan_id = row["scan"]
            file_path = row["mz_file_path"]
            if file_path not in file_to_scans:
                file_to_scans[file_path] = []
            file_to_scans[file_path].append(scan_id)

        exp_spectra_dfs = []
        for file_path, scan_ids in file_to_scans.items():
            logger.info(f"Extracting {len(scan_ids)} scans from {file_path}")
            spectra_df = extract_mzml_data(file_path, scan_ids)
            spectra_df["mz_file_path"] = file_path
            exp_spectra_dfs.append(spectra_df)

        # Merge spectral data from all files
        if exp_spectra_dfs:
            exp_spectra_df = pd.concat(exp_spectra_dfs, ignore_index=True)
            logger.info(
                f"Successfully extracted {len(exp_spectra_df)} experimental spectra"
            )
            return exp_spectra_df
        else:
            logger.warning("No experimental spectral data found")
            return pd.DataFrame()

    def _predict_theoretical_spectra(
        self, processed_peptides: List[str], charges: List[int]
    ) -> pd.DataFrame:
        """
        Use Koina to predict theoretical spectra.

        Parameters
        ----------
        processed_peptides : list of str
            List of preprocessed peptide sequences.
        charges : list of int
            List of charge states.

        Returns
        -------
        pd.DataFrame
            DataFrame containing predicted spectral data.

        Raises
        ------
        Exception
            If there is an error during Koina prediction.

        Notes
        -----
        The method uses Koina to predict theoretical spectra for each peptide.
        For AlphaPeptDeep_ms2_generic model, inputs are split into batches
        grouped by peptide length for prediction.
        """
        logger.info(f"Predicting theoretical spectra using {self.model_type}...")

        inputs = pd.DataFrame()
        inputs["peptide_sequences"] = np.array(processed_peptides)
        inputs["precursor_charges"] = np.array(charges)

        if self.collision_energies is not None:
            inputs["collision_energies"] = np.array(self.collision_energies)

        if self.instruments is not None:
            inputs["instrument_types"] = np.array(self.instruments)

        if self.fragmentation_types is not None:
            inputs["fragmentation_types"] = np.array(self.fragmentation_types)

        model = Koina(self.model_type, self.url)

        try:
            predictions = model.predict(inputs)
        except Exception as e:
            logger.error(f"Error during Koina prediction: {e}")
            logger.error("Koina prediction failed. Please check:")
            logger.error("- Input parameters compatibility")
            logger.error("- Supported modifications")
            logger.error("- Peptide length limits")
            logger.error(f"Details at: https://koina.proteomicsdb.org/")

            if self.model_type == "AlphaPeptDeep_ms2_generic":
                logger.info(
                    "Splitting inputs into batches grouped by peptide length for prediction..."
                )
                import re

                # Compute peptide length by removing modifications in square brackets.
                inputs["peptide_length"] = inputs["peptide_sequences"].apply(
                    lambda x: len(re.sub(r"\[.*?\]", "", x))
                )

                predictions_batches = []
                # Group the DataFrame by calculated peptide length. Each group is submitted as one batch.
                for peptide_length, group in inputs.groupby("peptide_length"):
                    logger.info(
                        f"Processing {len(group)} entries for peptide length {peptide_length}."
                    )
                    try:
                        batch_predictions = model.predict(group)
                        predictions_batches.append(batch_predictions)
                    except Exception as batch_error:
                        logger.error(
                            f"Error during Koina prediction for batch with peptide length {peptide_length}: {batch_error}"
                        )
                        logger.error(f"Batch data:\n{group}")
                        # # Save the batch data for debugging
                        # group.to_csv(
                        #     f"batch_error_peptide_length_{peptide_length}.csv",
                        #     index=False,
                        # )
                        # logger.error(
                        #     f"Batch data saved to batch_error_peptide_length_{peptide_length}.csv"
                        # )
                        raise
                predictions = pd.concat(predictions_batches, ignore_index=True)
            else:
                raise ValueError(f"Unsupported model type: {self.model_type}")

            # Save the raw prediction results
            self._raw_predictions = predictions.copy()

            # Convert prediction results to a suitable format
            pred_df = predictions.copy()
            pred_df.rename(
                columns={
                    "peptide_sequences": "processed_peptide",
                    "precursor_charges": "charge",
                    "intensities": "pred_intensity",
                    "mz": "pred_mz",
                },
                inplace=True,
            )

            # Group by peptide and charge, convert predicted mz and intensity to lists
            grouped_df = (
                pred_df.groupby(["processed_peptide", "charge"])
                .agg({"pred_intensity": list, "pred_mz": list, "annotation": list})
                .reset_index()
            )

            logger.info(f"Successfully predicted {len(grouped_df)} theoretical spectra")
            return grouped_df

    @property
    def raw_predictions(self) -> pd.DataFrame:
        """
        Returns the raw prediction results from Koina.

        Returns:
            pd.DataFrame: Raw prediction results DataFrame
        """
        if self._raw_predictions is None:
            if self.results is None:
                self.results = self._generate_features()
        return self._raw_predictions

    def get_raw_predictions(self) -> pd.DataFrame:
        """
        Get the raw prediction results DataFrame from Koina.

        Returns:
            pd.DataFrame: Raw prediction results DataFrame
        """
        return self.raw_predictions

    def save_raw_predictions(self, file_path: str, **kwargs) -> None:
        """
        Save the raw prediction results to a file.

        Parameters:
            file_path (str): Path to save the file
            **kwargs: Other parameters passed to pandas.DataFrame.to_csv
        """
        if "index" not in kwargs:
            kwargs["index"] = False
        if self.raw_predictions is not None:
            self.raw_predictions.to_csv(file_path, **kwargs)
            logger.info(f"Raw prediction results saved to: {file_path}")
        else:
            logger.warning("No raw prediction results available to save.")

    def _sort_spectrum_by_mz(
        self,
        mz: List[float],
        intensity: List[float],
        annotation: Optional[List[str]] = None,
    ) -> Tuple[np.ndarray, np.ndarray, Optional[np.ndarray]]:
        """
        Sort spectrum (m/z, intensity, and optionally annotation) by m/z values.

        Parameters
        ----------
        mz : list of float
            m/z values.
        intensity : list of float
            Intensity values.
        annotation : list of str, optional
            Fragment annotations.

        Returns
        -------
        tuple of (np.ndarray, np.ndarray, np.ndarray or None)
            Sorted m/z, intensity, and annotation arrays.
            If annotation is None, the third element will be None.
        """
        mz_array = np.array(mz)
        intensity_array = np.array(intensity)
        sorted_indices = np.argsort(mz_array)
        sorted_mz = mz_array[sorted_indices]
        sorted_intensity = intensity_array[sorted_indices]

        sorted_annotation = None
        if annotation is not None:
            annotation_array = np.array(annotation)
            sorted_annotation = annotation_array[sorted_indices]

        return sorted_mz, sorted_intensity, sorted_annotation

    def _align_spectra_all_peaks(
        self,
        exp_mz: List[float],
        exp_intensity: List[float],
        pred_mz: List[float],
        pred_intensity: List[float],
        pred_annotation: Optional[List[str]] = None,
        use_ppm: bool = True,
    ) -> Tuple[np.ndarray, np.ndarray, List[Tuple], Dict]:
        """
        Align experimental and predicted spectra.

        Parameters:
            exp_mz (List[float]): Experimental m/z values
            exp_intensity (List[float]): Experimental intensity values
            pred_mz (List[float]): Predicted m/z values
            pred_intensity (List[float]): Predicted intensity values
            pred_annotation (Optional[List[str]]): Predicted fragment annotations
            use_ppm (bool): Whether to use ppm tolerance or Da tolerance

        Returns:
            Tuple[np.ndarray, np.ndarray, List[Tuple], Dict]:
                - Aligned experimental intensity vector
                - Predicted intensity vector
                - Matching index pairs
                - Additional info including original sorted arrays
        """
        # Sort both experimental and predicted spectra by m/z
        exp_mz_sorted, exp_intensity_sorted, _ = self._sort_spectrum_by_mz(
            exp_mz, exp_intensity
        )

        if pred_annotation is not None:
            pred_mz_sorted, pred_intensity_sorted, pred_annotation_sorted = (
                self._sort_spectrum_by_mz(pred_mz, pred_intensity, pred_annotation)
            )
        else:
            pred_mz_sorted, pred_intensity_sorted, pred_annotation_sorted = (
                self._sort_spectrum_by_mz(pred_mz, pred_intensity)
            )
        aligned_exp_intensity = np.zeros(len(pred_mz_sorted))
        aligned_pred_intensity = pred_intensity_sorted.copy()
        matched_indices = []

        start_pos = 0

        for i, pred_peak_mz in enumerate(pred_mz_sorted):
            if use_ppm:
                fragment_min = pred_peak_mz * (1 - self.tolerance_ppm / 1e6)
                fragment_max = pred_peak_mz * (1 + self.tolerance_ppm / 1e6)
            else:
                # If using Da tolerance, use a fixed window (typically ~0.05 Da)
                tolerance = 0.05  # Default value
                fragment_min = pred_peak_mz - tolerance
                fragment_max = pred_peak_mz + tolerance

            matched_int = 0
            matched_exp_idx = None
            past_start = 0

            while start_pos + past_start < len(exp_mz_sorted):
                exp_peak_mz = exp_mz_sorted[start_pos + past_start]
                if exp_peak_mz < fragment_min:
                    start_pos += 1
                elif exp_peak_mz <= fragment_max:
                    exp_peak_int = exp_intensity_sorted[start_pos + past_start]
                    if exp_peak_int > matched_int:
                        matched_int = exp_peak_int
                        matched_exp_idx = start_pos + past_start
                    past_start += 1
                else:
                    break

            aligned_exp_intensity[i] = matched_int

            # Record matching index pairs (pred_idx, exp_idx)
            pred_idx = i
            exp_idx = matched_exp_idx
            matched_indices.append((pred_idx, exp_idx))

        additional_info = {
            "exp_mz_sorted": exp_mz_sorted,
            "exp_intensity_sorted": exp_intensity_sorted,
            "pred_mz_sorted": pred_mz_sorted,
            "pred_intensity_sorted": pred_intensity_sorted,
            "pred_annotation_sorted": pred_annotation_sorted,
        }

        return (
            aligned_exp_intensity,
            aligned_pred_intensity,
            matched_indices,
            additional_info,
        )

    def _get_top_peaks_vectors(
        self,
        aligned_exp_intensity: np.ndarray,
        aligned_pred_intensity: np.ndarray,
        matched_indices: List[Tuple],
        top_n: int,
    ) -> Tuple[np.ndarray, np.ndarray, List[Tuple]]:
        """
        Extract top N peaks based on predicted intensity for similarity calculation

        Parameters:
            aligned_exp_intensity (np.ndarray): Aligned experimental intensity vector
            aligned_pred_intensity (np.ndarray): Aligned predicted intensity vector
            matched_indices (List[Tuple]): Matching index pairs (pred_idx, exp_idx)
            top_n (int): Number of top peaks to extract

        Returns:
            Tuple[np.ndarray, np.ndarray, List[Tuple]]:
                - Top N experimental intensity vector
                - Top N predicted intensity vector
                - Top N matching index pairs
        """
        # Get indices of top N peaks by predicted intensity
        num_peaks = min(top_n, len(aligned_pred_intensity))
        top_pred_indices = np.argsort(-aligned_pred_intensity)[:num_peaks]

        # Extract top N peaks
        top_exp_intensity = np.zeros(num_peaks)
        top_pred_intensity = np.zeros(num_peaks)
        top_matched_indices = []

        for i, orig_idx in enumerate(top_pred_indices):
            top_exp_intensity[i] = aligned_exp_intensity[orig_idx]
            top_pred_intensity[i] = aligned_pred_intensity[orig_idx]
            top_matched_indices.append(matched_indices[orig_idx])

        return top_exp_intensity, top_pred_intensity, top_matched_indices

    def _align_spectra(
        self,
        exp_mz: List[float],
        exp_intensity: List[float],
        pred_mz: List[float],
        pred_intensity: List[float],
        pred_annotation: Optional[List[str]] = None,
    ) -> Tuple[np.ndarray, np.ndarray, List[Tuple]]:
        """
        Align experimental and predicted spectra.

        This is a wrapper around _align_spectra_all_peaks and _get_top_peaks_vectors
        to maintain backward compatibility while using the improved algorithm.

        Parameters
        ----------
        exp_mz : list of float
            Experimental m/z values.
        exp_intensity : list of float
            Experimental intensity values.
        pred_mz : list of float
            Predicted m/z values.
        pred_intensity : list of float
            Predicted intensity values.
        pred_annotation : list of str, optional
            Predicted fragment annotations.

        Returns
        -------
        tuple of (np.ndarray, np.ndarray, list of tuple)
            - Aligned experimental intensity vector
            - Predicted intensity vector
            - Matching index pairs (for top N peaks)

        Notes
        -----
        The method first aligns all peaks using _align_spectra_all_peaks, then
        extracts the top N peaks using _get_top_peaks_vectors for compatibility
        with existing code.
        """
        # First align all peaks
        all_exp_intensity, all_pred_intensity, all_matched_indices, additional_info = (
            self._align_spectra_all_peaks(
                exp_mz,
                exp_intensity,
                pred_mz,
                pred_intensity,
                pred_annotation,
                use_ppm=True,
            )
        )

        # Then get top N peaks for compatibility with existing code
        top_exp_intensity, top_pred_intensity, top_matched_indices = (
            self._get_top_peaks_vectors(
                all_exp_intensity, all_pred_intensity, all_matched_indices, self.top_n
            )
        )

        return top_exp_intensity, top_pred_intensity, top_matched_indices

    def _normalize_vector_l2(self, vector: np.ndarray) -> np.ndarray:
        """
        Normalize a vector using L2 normalization (unit vector).

        Parameters
        ----------
        vector : np.ndarray
            Input vector to normalize.

        Returns
        -------
        np.ndarray
            L2-normalized vector.

        Notes
        -----
        If the input vector has zero norm, the original vector is returned unchanged.
        """
        norm = np.linalg.norm(vector)  # Calculate the L2 norm (Euclidean norm)
        if norm == 0:
            return vector
        return vector / norm

    def _normalize_vector_sum(self, vector: np.ndarray) -> np.ndarray:
        """
        Perform sum normalization (probability normalization) on a vector.

        Parameters
        ----------
        vector : np.ndarray
            Input vector.

        Returns
        -------
        np.ndarray
            Sum normalized vector (sum to 1).

        Notes
        -----
        If the input vector has zero sum, the original vector is returned unchanged.
        """
        total = np.sum(vector)
        if total > 0:
            return vector / total
        return vector

    def _calculate_spectral_angle_similarity(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> float:
        """
        Calculate the spectral angle between experimental and predicted vectors.

        Normalize the angle to [0, 1] where 1 is the best similarity.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        float
            Spectral angle similarity (0-1, higher is better).

        Notes
        -----
        The spectral angle is calculated as: SA = 1 - (2 * angle / π),
        where angle is the angle between the normalized vectors in radians.
        """
        exp_norm = self._normalize_vector_l2(exp_vector)
        pred_norm = self._normalize_vector_l2(pred_vector)
        dot_product = np.sum(exp_norm * pred_norm)

        # Clamp dot product to [-1, 1] to avoid numerical issues
        dot_product = np.clip(dot_product, -1.0, 1.0)
        angle = np.arccos(dot_product)  # Angle in radians

        # Return similarity score: SA = 1 - (2 * angle / π)
        return 1 - (2 * angle / np.pi)

    def _calculate_cosine_similarity(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> float:
        """
        Calculate the cosine similarity between experimental and predicted vectors.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        float
            Cosine similarity (0-1, higher is better).

        Notes
        -----
        The cosine similarity is calculated as 1 - cosine_distance.
        If either vector has zero sum, returns 0.0.
        """
        if np.sum(exp_vector) == 0 or np.sum(pred_vector) == 0:
            return 0.0

        # Normalize vectors using L2 normalization
        exp_norm = self._normalize_vector_l2(exp_vector)
        pred_norm = self._normalize_vector_l2(pred_vector)

        # Use 1 - cosine distance = cosine similarity
        return 1.0 - cosine(exp_norm, pred_norm)

    def _calculate_spearman_correlation(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> float:
        """
        Calculate Spearman correlation coefficient between experimental and predicted vectors.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        float
            Spearman correlation coefficient (-1 to 1, higher is better).

        Notes
        -----
        If either vector has no variation (std = 0), returns 0.0.
        """
        # Handle vectors with no variation
        if np.std(exp_vector) == 0 or np.std(pred_vector) == 0:
            return 0.0

        try:
            r, _ = spearmanr(exp_vector, pred_vector)
            return r
        except:
            return

    def _calculate_pearson_correlation(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> float:
        """
        Calculate Pearson correlation coefficient between experimental and predicted vectors.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        float
            Pearson correlation coefficient (-1 to 1, higher is better).

        Notes
        -----
        If either vector has no variation (std = 0), returns 0.0.
        """
        # Handle vectors with no variation
        if np.std(exp_vector) == 0 or np.std(pred_vector) == 0:
            return 0.0

        try:
            r, _ = pearsonr(exp_vector, pred_vector)
            return r
        except:
            return 0.0

    def _calculate_mean_squared_error(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> float:
        """
        Calculate mean squared error between experimental and predicted vectors.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        float
            Mean squared error (lower is better).

        Notes
        -----
        The vectors are normalized using L2 normalization before calculating
        the mean squared error for fair comparison.
        """
        # Normalize vectors for fair comparison using L2 normalization
        exp_norm = self._normalize_vector_l2(exp_vector)
        pred_norm = self._normalize_vector_l2(pred_vector)

        return np.mean((exp_norm - pred_norm) ** 2)

    def _calculate_entropy(self, vector: np.ndarray) -> float:
        """
        Calculate Shannon entropy of a vector that has already been sum-1 normalized.

        Parameters
        ----------
        vector : np.ndarray
            Input vector, which has already been sum-1 normalized.

        Returns
        -------
        float
            Shannon entropy.

        Notes
        -----
        Only non-zero probabilities are considered for entropy calculation.
        If all probabilities are zero, returns 0.0.
        """
        # Only consider non-zero probabilities for entropy calculation
        mask = vector > 0
        if not np.any(mask):
            return 0.0

        prob_vector = vector[mask]
        return -np.sum(prob_vector * np.log(prob_vector))

    def _calculate_unweighted_entropy_similarity(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> float:
        """
        Calculate unweighted spectral entropy between experimental and predicted vectors.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        float
            Spectral entropy similarity.

        Notes
        -----
        Based on the method described in https://www.nature.com/articles/s41592-021-01331-z.
        The spectral entropy is calculated using the formula:
        1 - (2*S_PM - S_P - S_M)/ln(4), where S_PM is the entropy of the mixed
        distribution, and S_P and S_M are the entropies of the individual distributions.
        """
        # Sum-to-1 normalization
        sum_normalized_exp_vector = self._normalize_vector_sum(exp_vector)
        sum_normalized_pred_vector = self._normalize_vector_sum(pred_vector)
        s_exp = self._calculate_entropy(sum_normalized_exp_vector)
        s_pred = self._calculate_entropy(sum_normalized_pred_vector)
        s_mixed = self._calculate_entropy(
            0.5 * (sum_normalized_exp_vector + sum_normalized_pred_vector)
        )

        # Calculate spectral entropy using formula: 1 - (2*S_PM - S_P - S_M)/ln(4)
        unweighted_entropy_similarity = 1.0 - (2 * s_mixed - s_exp - s_pred) / np.log(4)

        return unweighted_entropy_similarity

    # TODO: Assign weight to each peak based on the entropy of a spectrum.
    # The original paper uses 3 as a entropy cutoff to assign more weight to the low intensity peaks.
    # But the cutoff value is determined by a small-molecular dataset rather than a proteomics dataset.
    # Thus, we need to design our own heuristic algorithm to calculate a similar feature for proteomics dataset.
    # We can refer to the practice of MSBooster.
    # url: https://github.com/Nesvilab/MSBooster/blame/master/src/main/java/features/spectra/SpectrumComparison.java#L803

    # def _calculate_entropy_similarity(self, exp_vector: np.ndarray, pred_vector: np.ndarray) -> float:

    def _calculate_predicted_counts(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> Tuple[int, int]:
        """
        Calculate counts of predicted peaks seen/not seen in experimental spectrum.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        tuple of (int, int)
            - predicted_seen_nonzero: Number of predicted peaks that are also present in experimental spectrum
            - predicted_not_seen: Number of predicted peaks that are not present in experimental spectrum
        """
        predicted_seen_nonzero = np.sum((pred_vector > 0) & (exp_vector > 0))
        predicted_not_seen = np.sum((pred_vector > 0) & (exp_vector == 0))

        return predicted_seen_nonzero, predicted_not_seen

    def _calculate_bray_curtis_similarity(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> float:
        """
        Calculate Bray-Curtis similarity between experimental and predicted vectors

        Parameters:
            exp_vector (np.ndarray): Experimental intensity vector
            pred_vector (np.ndarray): Predicted intensity vector

        Returns:
            float: Bray-Curtis similarity (0-1, higher is better)
        """
        # Bray-Curtis dissimilarity: sum(|exp - pred|) / sum(exp + pred)
        numerator = np.sum(np.abs(exp_vector - pred_vector))
        denominator = np.sum(exp_vector + pred_vector)

        if denominator == 0:
            return 0.0

        # Convert dissimilarity to similarity: 1 - dissimilarity
        return 1.0 - (numerator / denominator)

    def _calculate_similarity_features(
        self, exp_vector: np.ndarray, pred_vector: np.ndarray
    ) -> Dict[str, float]:
        """
        Calculate all similarity features between experimental and predicted vectors.

        Parameters
        ----------
        exp_vector : np.ndarray
            Experimental intensity vector.
        pred_vector : np.ndarray
            Predicted intensity vector.

        Returns
        -------
        dict of str to float
            Dictionary of similarity features, including:
            - spectral_angle_similarity: Spectral angle similarity (0-1)
            - cosine_similarity: Cosine similarity (0-1)
            - pearson_correlation: Pearson correlation (-1 to 1)
            - spearman_correlation: Spearman correlation (-1 to 1)
            - mean_squared_error: Mean squared error
            - unweighted_entropy_similarity: Spectral entropy similarity
            - predicted_seen_nonzero: Number of predicted peaks seen in experimental spectrum
            - predicted_not_seen: Number of predicted peaks not seen in experimental spectrum
            - bray_curtis_similarity: Bray-Curtis similarity (0-1)
        """
        spectral_angle_similarity = self._calculate_spectral_angle_similarity(
            exp_vector, pred_vector
        )
        cosine_similarity = self._calculate_cosine_similarity(exp_vector, pred_vector)
        pearson_correlation = self._calculate_pearson_correlation(
            exp_vector, pred_vector
        )
        spearman_correlation = self._calculate_spearman_correlation(
            exp_vector, pred_vector
        )
        mean_squared_error = self._calculate_mean_squared_error(exp_vector, pred_vector)
        unweighted_entropy_similarity = self._calculate_unweighted_entropy_similarity(
            exp_vector, pred_vector
        )
        predicted_seen_nonzero, predicted_not_seen = self._calculate_predicted_counts(
            exp_vector, pred_vector
        )
        bray_curtis_similarity = self._calculate_bray_curtis_similarity(
            exp_vector, pred_vector
        )

        return {
            "spectral_angle_similarity": spectral_angle_similarity,
            "cosine_similarity": cosine_similarity,
            "pearson_correlation": pearson_correlation,
            "spearman_correlation": spearman_correlation,
            "mean_squared_error": mean_squared_error,
            "unweighted_entropy_similarity": unweighted_entropy_similarity,
            "predicted_seen_nonzero": predicted_seen_nonzero,
            "predicted_not_seen": predicted_not_seen,
            "bray_curtis_similarity": bray_curtis_similarity,
        }

    def _generate_features(self) -> pd.DataFrame:
        """
        Generate spectral similarity features

        Returns:
            pd.DataFrame: DataFrame containing generated features
        """
        psm_df = self.df.copy()
        pred_spectra_df = self._predict_theoretical_spectra(
            processed_peptides=psm_df["processed_peptide"].tolist(),
            charges=psm_df["charge"].tolist(),
        )

        exp_spectra_df = self._extract_experimental_spectra()

        if not exp_spectra_df.empty:
            psm_df = pd.merge(
                psm_df,
                exp_spectra_df,
                on=["scan", "mz_file_path", "charge"],
                how="inner",
                validate="m:1",
            )
        else:
            logger.error(
                "Could not extract experimental spectral data, cannot continue processing"
            )
            return pd.DataFrame()

        if len(psm_df) != len(self.df):
            logger.warning("Some PSMs were not found in experimental spectral data")

        psm_df = pd.merge(
            psm_df, pred_spectra_df, on=["processed_peptide", "charge"], how="inner"
        )
        results = []

        logger.info(
            "Matching experimental and predicted spectra... This may take a while."
        )
        for _, row in psm_df.iterrows():
            exp_mz = row["mz"]
            exp_intensity = row["intensity"]
            pred_mz = row["pred_mz"]
            pred_intensity = row["pred_intensity"]
            pred_annotation = row["annotation"] if "annotation" in row else None

            # Align all peaks
            (
                all_exp_intensity,
                all_pred_intensity,
                all_matched_indices,
                additional_info,
            ) = self._align_spectra_all_peaks(
                exp_mz,
                exp_intensity,
                pred_mz,
                pred_intensity,
                pred_annotation,
                use_ppm=True,
            )

            # Extract top N peaks for similarity calculations
            top_exp_intensity, top_pred_intensity, top_matched_indices = (
                self._get_top_peaks_vectors(
                    all_exp_intensity,
                    all_pred_intensity,
                    all_matched_indices,
                    self.top_n,
                )
            )

            similarity_features = self._calculate_similarity_features(
                top_exp_intensity, top_pred_intensity
            )
            result = {
                "exp_vector": all_exp_intensity.tolist(),
                "pred_vector": all_pred_intensity.tolist(),
                "matched_indices": all_matched_indices,
                "exp_top_vector": top_exp_intensity.tolist(),
                "pred_top_vector": top_pred_intensity.tolist(),
                "top_matched_indices": top_matched_indices,
                **similarity_features,
                "exp_mz_sorted": additional_info["exp_mz_sorted"].tolist(),
                "exp_intensity_sorted": additional_info[
                    "exp_intensity_sorted"
                ].tolist(),
                "pred_mz_sorted": additional_info["pred_mz_sorted"].tolist(),
                "pred_intensity_sorted": additional_info[
                    "pred_intensity_sorted"
                ].tolist(),
            }

            # Add annotations if available
            if additional_info["pred_annotation_sorted"] is not None:
                result["pred_annotation_sorted"] = additional_info[
                    "pred_annotation_sorted"
                ].tolist()

            results.append(result)

        results_df = pd.DataFrame(results)
        psm_df = pd.concat(
            [psm_df.reset_index(drop=True), results_df.reset_index(drop=True)], axis=1
        )

        result_columns = [
            "mz_file_path",
            "spectrum_id",
            "scan",
            "peptide",
            "charge",
            "processed_peptide",
            "mz",
            "intensity",
            "pred_mz",
            "pred_intensity",
            "annotation",
            "exp_mz_sorted",
            "exp_intensity_sorted",
            "pred_mz_sorted",
            "pred_intensity_sorted",
            (
                "pred_annotation_sorted"
                if "pred_annotation_sorted" in psm_df.columns
                else None
            ),
            "exp_vector",
            "pred_vector",
            "matched_indices",
            "exp_top_vector",
            "pred_top_vector",
            "top_matched_indices",
            "spectral_angle_similarity",
            "cosine_similarity",
            "pearson_correlation",
            "spearman_correlation",
            "mean_squared_error",
            "unweighted_entropy_similarity",
            "predicted_seen_nonzero",
            "predicted_not_seen",
        ]

        result_columns = [col for col in result_columns if col is not None]
        result_df = psm_df[result_columns]

        logger.info(
            f"Successfully generated spectral similarity features for {len(result_df)} PSMs"
        )
        return result_df

    def generate_features(self) -> pd.DataFrame:
        """
        Public interface for generating spectral similarity features.

        Returns
        -------
        pd.DataFrame
            DataFrame containing the generated features.

        Notes
        -----
        This method is a wrapper around _generate_features that ensures
        the results are cached and only computed once.
        """
        if self.results is None:
            self.results = self._generate_features()
        return self.results[self.id_column + self.feature_columns]

    def get_full_data(self) -> pd.DataFrame:
        """
        Return the full DataFrame with all columns.

        Returns
        -------
        pd.DataFrame
            Full DataFrame with all columns.

        Notes
        -----
        This method returns the complete DataFrame including all intermediate
        results and raw data used in feature generation.
        """
        return self.results
